{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refine parallel dataset with character n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook guides you through a pipeline for refining a parallel dataset derived from standard token-to-token alignment models, to also include some likely character n-grams corresponding to the source word that original parallel dataset was based on. E.g., starting from a dataset containing each occurrence of the token 'when' in English and their parallels (translations) in several other languages, this notebook will calculate associations between 'when' and subtoken items to discover potential morphological means to express the meaning of 'when'. Where a language has both lexified counterparts to 'when' and morphological means (e.g. participles, converbs, next to 'when'-counterparts), the notebook will attempt to keep the counterpart while also grouping together sub-token units that show significant similarities at the character n-gram level. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current directory of the notebook\n",
    "notebook_dir = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "\n",
    "# Construct the path to the 'src' directory\n",
    "src_dir = os.path.join(notebook_dir, '../src')\n",
    "\n",
    "# Add the 'src' directory to sys.path\n",
    "if src_dir not in sys.path:\n",
    "    sys.path.append(src_dir)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "import spacy\n",
    "import spacy.cli\n",
    "\n",
    "# Check if the model is already downloaded\n",
    "if not spacy.util.is_package(\"en_core_web_sm\"):\n",
    "    # If not, download the model\n",
    "    spacy.cli.download(\"en_core_web_sm\")\n",
    "    \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Now you can import your modules\n",
    "from utils.text_processing import find_adv_head, transform_sentence, find_adv_head, remove_accents, extract_head_counterpart\n",
    "from analysis.ngram_analysis import NGramAssoc, source_target_stopwords_assoc\n",
    "\n",
    "# Load the English language model for dependency parsing\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need: \n",
    "1) A CSV file (to be defined in `advdf_path`), where each line is an occurrence of the source_token of interest in language A (e.g. English), and each column is the parallel in other languages, besides a `sent_id` column and a `context` column (with the source text).\n",
    "2) A list of target languages in their ISO- code, whose column you want to refine. You'll get both a copy of the original CSV file with the respective columns modified (and the others left untouched) and a copy of the same but only with the `sent_id`, `context`, source language, and target languages that were modified.\n",
    "3) The alignment models in a CSV format, with the first column being a `sent_id` that can be mapped to the CSV in `advdf_path`, the second column being a `context` column corresponding to the `sent_id` and the third column a mapping from source to target language context in the format word1 (parallel1), word2 (parallel2), etc. For instance, the header of the alignment model for [aai] is:\n",
    "\n",
    "```\n",
    "sent_id,context,targ\n",
    "40001003,and judah the father of perez and zerah by tamar and perez the father of hezron and hezron the father of ram,and (naatu) judah (judah) the (NOMATCH) father (NOMATCH) of (natun) perez (perez) and (naatu) zerah (zerah) by (NOMATCH) tamar (natunatun) and (naatu) perez (perez) the (NOMATCH) father (NOMATCH) of (natun) hezron (NOMATCH) and (naatu) hezron (NOMATCH) the (NOMATCH) father (NOMATCH) of (natun) ram (ram) \n",
    "```\n",
    "\n",
    "The location of the alignment models is to be defined in `alignments_parent_path` below.\n",
    "\n",
    ":warning: Make sure that 'empty'/null-token alignments are marked as 'NOMATCH' rather than 'NULL' or other variations, to clearly distinguish them from empty parallels due to the lack of target text (as opposed to lack of a lexical counterpart).\n",
    "\n",
    "You can define some stopwords, to ensure they are not mistaken as parallels to your word of interest if they occur very often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the source_token and target languages for processing\n",
    "source_token = 'when'\n",
    "targetlangs = [\"aca\",\"acf\",\"acr\",\"acu\",\"agr\",\"agu\",\"ake\",\"ame\",\"amr\",\"amu\",\"apn\",\"apu\",\"apy\",\"arl\",\"arn\",\"auc\",\"ayo\",\"ayr\",\"azg\",\"azz\",\"bao\",\"bkq\",\"bmr\",\"boa\",\"bsn\",\"bzd\",\"bzj\",\"caa\",\"cab\",\"cac\",\"cag\",\"cak\",\"cao\",\"cap\",\"car\",\"cas\",\"cav\",\"cax\",\"cbc\",\"cbi\",\"cbr\",\"cbs\",\"cbt\",\"cbu\",\"cbv\",\"cco\",\"ceg\",\"chd\",\"chf\",\"chq\",\"chz\",\"cjo\",\"cjp\",\"cle\",\"cly\",\"cni\",\"cnl\",\"cnt\",\"coe\",\"cof\",\"cok\",\"con\",\"cot\",\"cpa\",\"cpb\",\"cpc\",\"cpu\",\"cpy\",\"crn\",\"crq\",\"crt\",\"cso\",\"cta\",\"ctp\",\"ctu\",\"cub\",\"cuc\",\"cui\",\"cuk\",\"cul\",\"cut\",\"cux\",\"cya\",\"des\",\"djk\",\"emp\",\"enx\",\"ese\",\"gnw\",\"gub\",\"guc\",\"gug\",\"guh\",\"gui\",\"gum\",\"gun\",\"guo\",\"guq\",\"gvc\",\"gym\",\"gyr\",\"hat\",\"hch\",\"hix\",\"hns\",\"hto\",\"hub\",\"hus\",\"huu\",\"huv\",\"icr\",\"ign\",\"inb\",\"ixl\",\"jac\",\"jam\",\"jic\",\"jiv\",\"jvn\",\"kaq\",\"kbc\",\"kbh\",\"kek\",\"kgk\",\"kgp\",\"kjb\",\"knj\",\"kog\",\"kpj\",\"kvn\",\"kwi\",\"kyz\",\"lac\",\"leg\",\"maa\",\"maj\",\"mam\",\"maq\",\"mau\",\"mav\",\"maz\",\"mbc\",\"mbj\",\"mbl\",\"mca\",\"mcb\",\"mcd\",\"mcf\",\"mco\",\"mfy\",\"mib\",\"mie\",\"mig\",\"mih\",\"mil\",\"mio\",\"miq\",\"mir\",\"mit\",\"miy\",\"miz\",\"mjc\",\"mks\",\"moc\",\"mop\",\"mpm\",\"mto\",\"mtp\",\"mxb\",\"mxp\",\"mxq\",\"mxt\",\"mxv\",\"myu\",\"myy\",\"mza\",\"mzh\",\"mzl\",\"nab\",\"nch\",\"ncj\",\"ncl\",\"ngu\",\"nhd\",\"nhe\",\"nhg\",\"nhi\",\"nhw\",\"nhx\",\"nhy\",\"noa\",\"not\",\"npl\",\"nsu\",\"ntp\",\"ote\",\"otm\",\"otn\",\"otq\",\"ots\",\"oym\",\"pab\",\"pad\",\"pah\",\"pap\",\"pbb\",\"pbc\",\"pib\",\"pio\",\"pir\",\"plg\",\"pls\",\"plu\",\"poe\",\"poh\",\"poi\",\"pps\",\"pua\",\"qub\",\"quc\",\"quf\",\"qug\",\"quh\",\"qul\",\"qup\",\"quw\",\"quy\",\"quz\",\"qva\",\"qvc\",\"qve\",\"qvh\",\"qvi\",\"qvm\",\"qvn\",\"qvo\",\"qvs\",\"qvw\",\"qvz\",\"qwh\",\"qxh\",\"qxl\",\"qxn\",\"qxo\",\"qxr\",\"rkb\",\"sab\",\"sey\",\"shp\",\"sja\",\"snn\",\"sri\",\"srm\",\"srn\",\"srq\",\"stp\",\"tac\",\"tar\",\"tav\",\"tca\",\"tee\",\"ter\",\"tfr\",\"tku\",\"tna\",\"tnc\",\"tob\",\"toc\",\"toj\",\"too\",\"top\",\"tos\",\"tpp\",\"tpt\",\"tqb\",\"trc\",\"trn\",\"trq\",\"tsz\",\"ttc\",\"tue\",\"tuf\",\"tuo\",\"txu\",\"tzh\",\"tzj\",\"tzo\",\"ura\",\"urb\",\"usp\",\"var\",\"vmy\",\"wap\",\"way\",\"wca\",\"xav\",\"xsu\",\"xtd\",\"xtm\",\"xtn\",\"yaa\",\"yad\",\"yan\",\"yaq\",\"ycn\",\"yua\",\"yuz\",\"zaa\",\"zab\",\"zac\",\"zad\",\"zae\",\"zai\",\"zam\",\"zao\",\"zar\",\"zas\",\"zat\",\"zav\",\"zaw\",\"zca\",\"zos\",\"zpc\",\"zpi\",\"zpl\",\"zpm\",\"zpo\",\"zpq\",\"zpt\",\"zpu\",\"zpv\",\"zpz\",\"zsr\",\"ztq\",\"zty\"]  # List of target language codes\n",
    "\n",
    "# Define paths for the dataframe and the alignment files\n",
    "advdf_path = '../datasets/when-latamecarr.csv'\n",
    "# The parent folder in which the all the CSV alignement files are found\n",
    "alignments_parent_path = '../datasets/symgizamodel'\n",
    "\n",
    "# Define a list of stopwords for source language\n",
    "stopwords_source = ['jesus','herod','paul','peter','and','behold','then']\n",
    "# Optional:\n",
    "# stopwords_target = ['же','δὲ']\n",
    "\n",
    "outputdir = '../outputs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will output a bunch of files for each target language:\n",
    "- [langname]-char-ngram-assoc.txt: a TSV file containing most likely character n-grams corresponding to the word of interest, ordered by p-value for the chi2 association and 'true positives', i.e. how many times a given ngram is found as translating the word of interest. \n",
    "- [langname]-word-assoc.txt: same measures as above, but at the word level.\n",
    "- [langname]-stopwords.txt: same measures as above, but containing the most likely parallels to the stopwords provided in `stopwords_source`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a file to write output related to ngrams\n",
    "with open(f'{outputdir}{source_token}-ngrams-details.txt','w') as outtxtgrams:\n",
    "    # Load the dataframe containing alignments\n",
    "    advdf = pd.read_csv(advdf_path,dtype=str)\n",
    "\n",
    "    # Collect all alignment CSV files from the specified folder\n",
    "    alignments_paths = sorted(glob(f'{alignments_parent_path}/*.csv'))\n",
    "\n",
    "    # Iterate through each alignment file\n",
    "    for alignments_path in alignments_paths:\n",
    "        language = alignments_path.split('/')[-1].split('-')[0]\n",
    "        if language in targetlangs:\n",
    "            try:\n",
    "                print(language)\n",
    "                # Read the current alignment file\n",
    "                df_with_parall = pd.read_csv(alignments_path,dtype=str)\n",
    "\n",
    "                # Merge the alignment data with the {words}-dataframe based on 'sent_id' and 'context'\n",
    "                df_with_parall = advdf.merge(df_with_parall, how='left',on=['sent_id','context'])\n",
    "\n",
    "                # Process 'context' column to find occurrences of the source_token\n",
    "                adv_heads_col = [find_adv_head(cont,source_token,nlp) for cont in df_with_parall['context']]\n",
    "\n",
    "                # Process the results to format them for output\n",
    "                df_with_parall['adv_head'] = [','.join(x) if x else None for x in adv_heads_col]\n",
    "\n",
    "                # Process each row to transform the sentence based on the target and headword\n",
    "                with open(f'{outputdir}{language}-{source_token}-with-adv-head.txt', 'w') as outtxt:\n",
    "                    for index, row in df_with_parall.iterrows():\n",
    "                        if not pd.isna(row['context']):\n",
    "                            target = remove_accents(str(row['targ']))\n",
    "                            headword = row['adv_head']\n",
    "                            output_sentence = transform_sentence(str(target),str(headword))\n",
    "                            outtxt.write(output_sentence + '\\n')\n",
    "\n",
    "                # Define target words for NGram alignment\n",
    "                target_words=[source_token,'advhead']\n",
    "\n",
    "                # Collect stopwords in the target language\n",
    "                res = source_target_stopwords_assoc(f'{outputdir}{language}-{source_token}-with-adv-head.txt', outputdir, stopwords_source, source_token, language)\n",
    "                \n",
    "                # Now read in the file with stopwords matches\n",
    "                df_sw= pd.read_csv(f'{outputdir}{language}-{source_token}-stopwords.txt',sep='\\t')\n",
    "                # Try and use only those with a p-value of 0\n",
    "                # Initialize stopwords_target if not provided by the user\n",
    "                if 'stopwords_target' not in locals():\n",
    "                    stopwords_target = []\n",
    "\n",
    "                # Add new stopwords to the list if the p-value is 0.0\n",
    "                stopwords_target.extend(df_sw[df_sw['p-value'] == 0.0]['feature'].tolist())\n",
    "\n",
    "                # If there are fewer words with such p-value than the number of stopwords, than order by p-value (ascending) and take the first n (= number of stopwords)\n",
    "                if len(stopwords_target) < len(stopwords_source):\n",
    "                    stopwords_target = list(set(list(df_sw[df_sw['p-value']==0.0]['feature']) + list(df_sw.sort_values(by='p-value')['feature'][0:len(stopwords_source)])))\n",
    "\n",
    "                res=NGramAssoc(f'{outputdir}{language}-{source_token}-with-adv-head.txt',outputdir,target_words,stopwords_source,stopwords_target,source_token,language)\n",
    "\n",
    "                df= pd.read_csv(f'{outputdir}{language}-{source_token}-word-assoc.txt',sep='\\t')\n",
    "                    \n",
    "                # Only take words matching source_token with a p-value of 0\n",
    "                source_token_words = []\n",
    "                perfect_pvalue = list(df[df['p-value'] == 0.0]['feature'])\n",
    "\n",
    "                if len(perfect_pvalue) == 0:\n",
    "                    best_match = list(df.sort_values(by=['score'],ascending=False)['feature'])[0]\n",
    "                    source_token_words.append(best_match)\n",
    "                else:\n",
    "                    source_token_words = source_token_words + perfect_pvalue\n",
    "\n",
    "                # Your DataFrame\n",
    "                df = pd.read_csv(f'{outputdir}{language}-{source_token}-char-ngram-assoc.txt', sep='\\t')\n",
    "\n",
    "                # Only consider ngrams occurring at the end of words (signaled by '@')\n",
    "                # Note that this is experimental, and by no means the best approach. Needs to be tested systematically against, e.g. templatic languages\n",
    "                df = df[(df['score'] > 1) & (df['feature'].str.contains('@'))].sort_values(by=['c11'], ascending=False).head(20)\n",
    "\n",
    "                # Calculate TF-IDF\n",
    "                vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 8))\n",
    "                tfidf_matrix = vectorizer.fit_transform(df['feature']).toarray()  # Convert to dense array\n",
    "\n",
    "                # Number of clusters/components\n",
    "                num_clusters_components = 3\n",
    "\n",
    "                # K-Means clustering with different initialization methods\n",
    "                np.random.seed(42)\n",
    "                random_state = 42  # Random state for reproducibility\n",
    "\n",
    "                kmeans_random = KMeans(n_clusters=num_clusters_components, init='random', random_state=random_state,n_init=10)\n",
    "                df['cluster_kmeans_random'] = kmeans_random.fit_predict(tfidf_matrix)\n",
    "\n",
    "                kmeans_kmeans_plus_plus = KMeans(n_clusters=num_clusters_components, init='k-means++', random_state=random_state,n_init=10)\n",
    "                df['cluster_kmeans_kmeans++'] = kmeans_kmeans_plus_plus.fit_predict(tfidf_matrix)\n",
    "\n",
    "                # DBSCAN clustering\n",
    "                dbscan = DBSCAN(eps=1, min_samples=3)\n",
    "                df['cluster_dbscan'] = dbscan.fit_predict(tfidf_matrix)\n",
    "\n",
    "                # Agglomerative clustering\n",
    "                agglomerative = AgglomerativeClustering(n_clusters=num_clusters_components)\n",
    "                df['cluster_agglomerative'] = agglomerative.fit_predict(tfidf_matrix)\n",
    "\n",
    "                # GMM clustering with fixed number of components\n",
    "                gmm = GaussianMixture(n_components=num_clusters_components, random_state=random_state)\n",
    "                df['cluster_gmm'] = gmm.fit_predict(tfidf_matrix)\n",
    "\n",
    "                # Print cluster assignments\n",
    "                # for algorithm, cluster_columns in [('K-Means (Random)', 'cluster_kmeans_random'),\n",
    "                #                                 ('K-Means (k-means++)', 'cluster_kmeans_kmeans++'),\n",
    "                #                                 ('DBSCAN', 'cluster_dbscan'),\n",
    "                #                                 ('Agglomerative', 'cluster_agglomerative'),\n",
    "                #                                 ('GMM', 'cluster_gmm')]:\n",
    "                    \n",
    "                for algorithm, cluster_columns in [('DBSCAN', 'cluster_dbscan')]:\n",
    "                    print(f\"\\n{algorithm} Clustering:\")\n",
    "                    for cluster_number in sorted(df[cluster_columns].unique()):\n",
    "                        # Select rows corresponding to the current cluster number\n",
    "                        cluster_rows = df[df[cluster_columns] == cluster_number]\n",
    "\n",
    "                        # Print the values in df['feature'] as a list for the current cluster\n",
    "                        feature_list = cluster_rows['feature'].tolist()\n",
    "                        print(f'Cluster {cluster_number}: {feature_list}')\n",
    "\n",
    "                # Apply the function to create a new column\n",
    "                df_with_parall['adv_head_transl'] = df_with_parall.apply(extract_head_counterpart, axis=1)\n",
    "                # print(df_with_parall)\n",
    "                ngrams_clusters = []\n",
    "                full_words = []\n",
    "\n",
    "                for name, group in df[df['cluster_dbscan'] >= 0].groupby('cluster_dbscan'):\n",
    "                    # print(f'Cluster {name}')\n",
    "                    cluster = []\n",
    "                    # print(group['feature'])\n",
    "                    for ngram in group['feature']:\n",
    "                        if '$' in ngram:\n",
    "                            newngram = ngram.split('$')[1].split('@')[0]\n",
    "                            # print(f'ngram {newngram} is full word')\n",
    "                            full_words.append(newngram)\n",
    "                            # cluster.append(newngram)\n",
    "                        else:\n",
    "                            newngram = ngram.split('@')[0]\n",
    "                            cluster.append(newngram)\n",
    "                    ngrams_clusters.append(cluster)\n",
    "\n",
    "                outtxtgrams.write(language)\n",
    "                outtxtgrams.write('\\n')\n",
    "                outtxtgrams.write('\\n')\n",
    "                for iclu in range(len(ngrams_clusters)):\n",
    "                    ngramcurrent = ', '.join(ngrams_clusters[iclu])\n",
    "                    clustern = iclu + 1\n",
    "                    outtxtgrams.write(f'ngram_{clustern}: {ngramcurrent}')\n",
    "                    outtxtgrams.write('\\n')\n",
    "\n",
    "                patterntofilter = language + '-'\n",
    "\n",
    "                lang_names_with_code = advdf.filter(like=patterntofilter, axis=1).columns\n",
    "                \n",
    "                print('langnameswithcodes',lang_names_with_code)\n",
    "                for lang_name_with_code in lang_names_with_code:\n",
    "                    print('adding column')\n",
    "                    newcol = []\n",
    "                    for index,row in df_with_parall.iterrows():\n",
    "                        target = row['targ']\n",
    "                        # print(row[lang_name_with_code])\n",
    "                        if remove_accents(str(row[lang_name_with_code])) in source_token_words:\n",
    "                            newcol.append(remove_accents(str(row[lang_name_with_code])))\n",
    "                        elif row[lang_name_with_code] in full_words:\n",
    "                            newcol.append(row[lang_name_with_code])\n",
    "                        elif row[lang_name_with_code] == 'NOMATCH':\n",
    "                            # print(f'Checking adv_head grams for sentence {target}')\n",
    "                            anytrue = []\n",
    "                            for cluster_values in ngrams_clusters:\n",
    "                                ends_with_any = any(remove_accents(str(row['adv_head_transl'])).endswith(value) for value in cluster_values)\n",
    "                                # print(f\"Head word {row['adv_head_transl']} ends with any element in {cluster_values}: {ends_with_any}\")\n",
    "                            if True in anytrue:\n",
    "                                indexoftrue = anytrue.index(True)\n",
    "                                foundcluster = ngrams_clusters[indexoftrue]\n",
    "                                # print('Gram found')\n",
    "                                newcol.append(f'ngram_{indexoftrue + 1}')\n",
    "                            else:\n",
    "                                newcol.append('NOMATCH')\n",
    "                        else:\n",
    "                            # print('Checking when grams')\n",
    "                            anytrue = []\n",
    "                            for cluster_values in ngrams_clusters:\n",
    "                                ends_with_any = any(remove_accents(str(row[lang_name_with_code])).endswith(value) for value in cluster_values)\n",
    "                                anytrue.append(ends_with_any)\n",
    "                                # print(f\"Word {row[lang_name_with_code]} ends with any element in {cluster_values}: {ends_with_any}\")\n",
    "                            if True in anytrue:\n",
    "                                indexoftrue = anytrue.index(True)\n",
    "                                foundcluster = ngrams_clusters[indexoftrue]\n",
    "                                # print('Gram found')\n",
    "                                newcol.append(f'ngram_{indexoftrue + 1}')\n",
    "                                continue\n",
    "                            else:\n",
    "                                anytrue = []\n",
    "                                # print(f'Checking adv_head grams for sentence {target}')\n",
    "                                for cluster_values in ngrams_clusters:\n",
    "                                    ends_with_any = any(remove_accents(str(row['adv_head_transl'])).endswith(value) for value in cluster_values)\n",
    "                                    # print(f\"Head word {row['adv_head_transl']} ends with any element in {cluster_values}: {ends_with_any}\")\n",
    "                                    anytrue.append(ends_with_any)\n",
    "                                if True in anytrue:\n",
    "                                    indexoftrue = anytrue.index(True)\n",
    "                                    foundcluster = ngrams_clusters[indexoftrue]\n",
    "                                    # print('Gram found')\n",
    "                                    newcol.append(f'ngram_{indexoftrue + 1}')\n",
    "                                else:\n",
    "                                    if remove_accents(str(row[lang_name_with_code])) not in stopwords_target:\n",
    "                                        # print('Gram not found')\n",
    "                                        newcol.append(remove_accents(str(row[lang_name_with_code])))\n",
    "                                    else:\n",
    "                                        # print('Gram not found')\n",
    "                                        newcol.append('NOMATCH')\n",
    "\n",
    "                advdf[lang_name_with_code] = newcol\n",
    "            except ValueError:\n",
    "                advdf = advdf.drop(columns=lang_name_with_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save refined dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will output two new dataframes:\n",
    "- {source_token}_withgrams.csv: original parallel dataset, but with the columns for the target languages refined with n-grams.\n",
    "- {source_token}_withgrams_selectedcols.csv: original parallel dataset, but _only_ with the columns for the target languages refined with n-grams (besides context, sent_id, and source language)\n",
    "\n",
    "\n",
    "The former can be then used as usual to produce semantic maps (see other notebook).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "advdf.to_csv(f'{outputdir}{source_token}_withgrams.csv',index=False)\n",
    "\n",
    "selected_columns = advdf.loc[:, [col for col in advdf.columns if any(lang in col for lang in targetlangs)]].columns\n",
    "selected_columns = ['sent_id','context','eng-29'] + list(selected_columns)\n",
    "advdf.to_csv(f'{outputdir}{source_token}_withgrams_selectedcols.csv',index=False,columns=selected_columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ngram-typology-L59eT3NA-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
